{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 340 Lecture 33: Deep learning software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming languages\n",
    "\n",
    "- In ML we use a variety of programming languages: Python, Java, R, C++, Matlab, many more. \n",
    "  - In CPSC 340 we used Python so I'll mainly stick to Python packages in this lecture.\n",
    "  - Most deep learning packages are Python-based in any case.\n",
    "\n",
    "- A popular open-source library is [scikit-learn](http://scikit-learn.org/stable/), which we've used a few times in the course. \n",
    "\n",
    "- In Assignment 6 you'll use sklearn's [neural network implementation](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html). By the way, this was [written by your TA, Issam](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/neural_network/multilayer_perceptron.py#L4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning software\n",
    "\n",
    "There's been a lot of software released lately to take care of this for you. Some big players are:\n",
    "\n",
    "| Name   |  Host language  | Released |  Comments | \n",
    "|--------|-------------|---------------|----------|\n",
    "| [Theano](http://deeplearning.net/software/theano/) | Python | 2007 | From U. de MontrÃ©al |\n",
    "| [Torch](http://torch.ch) | Lua | 2002 | Used at Facebook |\n",
    "| [PyTorch](http://pytorch.org) | Python | 2017 | Automatic differentiation through arbitrary code like [Autograd](https://github.com/HIPS/autograd)\n",
    "| [TensorFlow](https://www.tensorflow.org) | Python | 2015 | Created by Google for both prototyping and production\n",
    "| [Keras](https://keras.io) | Python | 2015 | A front-end on top of Theano or TensorFlow, soon to be more integrated |\n",
    "| [Caffe](http://caffe.berkeleyvision.org) | Executable with Python wrapper | 2014 | Specifically for convnets (see Lectures 31-32), from UC Berkeley\n",
    "\n",
    "- There are many others of course. See for example [this Wikipedia article](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n",
    "- From the table above, we can see there have been a lot of new packages released recently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs\n",
    "\n",
    "- Part of the recent progress in deep learning has been due to the computational power of GPUs.\n",
    "- GPUs were originally designed for graphics, which requires a lot of _matrix multiplication_.\n",
    "  - This is exactly what we need for neural networks.\n",
    "- The leader is NVIDIA, an American company, and their GPU programming language is called CUDA.\n",
    "  - Luckily, we rarely need to write CUDA code anymore, as there are intermediate layers of software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud computing\n",
    "\n",
    "- I'm guessing few/none of the laptops in the room have a CUDA-capable NVIDIA GPU capable of fast training. \n",
    "  - My Macbook Air certainly isn't good for much in terms of computation.\n",
    "- Luckily, the advent of cloud computing platforms like Amazon's EC2 gives us easy access to computing resources.\n",
    "- Amazon also offers [Amazon Machine Images](https://en.wikipedia.org/wiki/Amazon_Machine_Image) (AMIs) which simplify things further. \n",
    "  - This is like a virtual machine and comes with relevant software installed, etc.\n",
    "  - The AMI costs a bit extra on top of the EC2 costs (say 10%)\n",
    "  - If you were a big company you would probably create your own, but these public AMIs are great for prototyping.\n",
    "  - If you're interested in containerization, [Docker](https://www.docker.com/) is gaining a lot of popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos: loading the data\n",
    "\n",
    "The data is built in to Keras, so we just access it for convenience. If not present already it is automatically downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train_cat), (X_test, y_test_cat) = mnist.load_data()\n",
    "\n",
    "img_dim = (28,28) \n",
    "img_size = img_dim[0]*img_dim[1]\n",
    "num_classes = 10\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test  = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test  /= 255\n",
    "X_train_flat = X_train.reshape(60000, img_size)\n",
    "X_test_flat  = X_test.reshape(10000, img_size)\n",
    "X_train = X_train[...,None] # add 4th dimension, needed later for convnets\n",
    "X_test  = X_test[...,None]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train_cat, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test_cat, num_classes)\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: scikit-learn on my laptop\n",
    "\n",
    "See documentation for the [classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) and [regressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.995783333333\n",
      "Test accuracy: 0.9761\n",
      "1 loop, best of 1: 30.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100,100), max_iter=10, batch_size=128)\n",
    "nn.fit(X_train_flat, y_train_cat)\n",
    "\n",
    "score = nn.score(X_train_flat, y_train_cat)\n",
    "print('Train accuracy:', score)\n",
    "\n",
    "score = nn.score(X_test_flat, y_test_cat)\n",
    "print('Test accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Keras/TensorFlow on my laptop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected net\n",
    "\n",
    "Attribution: the code below is adapted from the [Keras MNIST example](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py), which is under the [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Imports and preprocessing **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model definition **\n",
    "\n",
    "Here we need to specify the input and output size in advance (unlike sklearn) because the model is first _compiled_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_train_flat.shape[1],), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training and evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.993483333333\n",
      "Test accuracy: 0.9769\n",
      "1 loop, best of 1: 40 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "history = model.fit(X_train_flat, y_train,\n",
    "                    batch_size=128, \n",
    "                    nb_epoch=10,\n",
    "                    verbose=0)\n",
    "\n",
    "score = model.evaluate(X_train_flat, y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional net\n",
    "\n",
    "Attribution: the code below is adapted from [Deep Learning with Python](https://machinelearningmastery.com/deep-learning-with-python2/) with permission from the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model definition **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 5, 5, input_shape=img_dim+(1,), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training and evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.99475\n",
      "Test accuracy: 0.9893\n",
      "1 loop, best of 1: 1min 24s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128, nb_epoch=1,\n",
    "                    verbose=0, validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Keras/TensorFlow on EC2 (with GPU)\n",
    "\n",
    "- I'll now attempt to demo Keras/TensorFlow on EC2 \"from scratch\".\n",
    "  - By this I mean I haven't done much of the work in advance.\n",
    "  - You should be able to follow along if you already have an AWS account and you're willing to pay a few dollars.\n",
    "- This is probably a bad idea. I'll give the demo a 50% chance of working.\n",
    "  \n",
    "Here are the steps I'm about to do:\n",
    "\n",
    "1. (already done) sign into [AWS](https://aws.amazon.com).\n",
    "2. Go to the [AWS console](https://console.aws.amazon.com) and select EC2.\n",
    "3. Select \"Key Pairs\" on the left and create a new Key Pair.\n",
    "\n",
    "2. Visit the site of the [AMI I'm using](https://aws.amazon.com/marketplace/pp/B01EYKBEQ0).\n",
    "3. Press \"Continue\"\n",
    "4. Select US West (Oregon) as my [EC2 region](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html), and p2.xlarge as my [EC2 instance type](https://aws.amazon.com/ec2/instance-types/).\n",
    "5. Launch with 1-Click. The cost is \\$1/hour.\n",
    "5. Go back to the EC2 console and click Instances on the left side. \n",
    "6. Wait until the EC2 console lists the Instance State as \"running\"...\n",
    "7. Right-click the instance in the console, and press Connect. Follow the instructions there. \n",
    "  - change permissions on key\n",
    "  - `ssh` into the instance\n",
    "8. Run `watch -n 1 nvidia-smi` so I can monitor the GPU. You'll see it's a [Tesla K80](http://www.nvidia.com/object/tesla-k80.html), which is [not cheap](https://www.amazon.com/HP-J0G95A-NVIDIA-Tesla-K80/dp/B00TWFEIWA). It starts around body temperature.\n",
    "8. And now... go back to the console and get the public IP of the instance.\n",
    "9. Point my browser to `http://{EC2 Instance Public IP}:8888`\n",
    "10. Grab the instance id from the EC2 console: this is the password.\n",
    "11. Create a notebook, paste in the above code. And run!\n",
    "12. Go back to the terminal and see that the GPU is being utilized. Is the temperature going up?\n",
    "13. **IMPORTANT** Go back to the EC2 console, right-click on the instance, Instance State --> Terminate.\n",
    "\n",
    "Compared to my laptop, I observed a speedup of about **10x**. This translates into better models when training time is the limiting factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: spot instances\n",
    "\n",
    "- EC2 offers both \"dedicated instances\" and [\"spot instances\"](https://aws.amazon.com/ec2/spot/). \n",
    "- Spot instances and much cheaper but may be interrupted if the \"spot price\" goes above your maximum price. \n",
    "  - For example, our instance is about \\$1/hour and a spot instance would be about \\$0.30/hour\n",
    "- In general you should always use spot instances when running experiments because of price.\n",
    "  - You'd want dedicated instances if you were _deploying_ your trained model in an app, though!\n",
    "- I am not using a spot instance in this demo because they aren't supported with the \"1-Click Launch\" feature of this AMI.\n",
    "  - But if I wasn't doing a demo then I'd launch a spot instance myself, through the EC2 console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Amazon educate\n",
    "\n",
    "- Amazon has a program that gives [free credit for students](https://aws.amazon.com/education/awseducate/). \n",
    "- However, because UBC uses `@alumni.ubc.ca` email addresses for current students, the automated system tends to reject applications from UBC students.\n",
    "- If you have a CS ugrad account you can probably use it (but I think these are disappearing next year?).\n",
    "- We are working on this, maybe it will be solved by next year..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final advice: terminate your EC2 instances!!!\n",
    "\n",
    "- This may be the most important advice I give you in CPSC 340 ;)\n",
    "- If you forget to do this, you can be stuck with a big bill from Amazon.\n",
    "- You can do this from the EC2 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
