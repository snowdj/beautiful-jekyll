{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning software\n",
    "\n",
    "CPSC 340: Machine Learning and Data Mining\n",
    "\n",
    "The University of British Columbia\n",
    "\n",
    "2017 Winter Term 2\n",
    "\n",
    "Mike Gelbart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some popular packages:\n",
    "\n",
    "| Name   |  Host language  | Released |  Comments | \n",
    "|--------|-------------|---------------|----------|\n",
    "| [Torch](http://torch.ch) | Lua | 2002 | Early library, still used |\n",
    "| [Theano](http://deeplearning.net/software/theano/) | Python | 2007 | From U. de Montréal |\n",
    "| [Caffe](http://caffe.berkeleyvision.org) | Executable with Python wrapper | 2014 | Specifically for CNNs, from UC Berkeley\n",
    "| [TensorFlow](https://www.tensorflow.org) | Python | 2015 | Created by Google for both prototyping and production\n",
    "| [Keras](https://keras.io) | Python | 2015 | A front-end on top of Theano or TensorFlow |\n",
    "| [PyTorch](http://pytorch.org) | Python | 2017 | Flexible, gaining a lot of popularity\n",
    "| [Caffe 2](https://caffe2.ai/) | Python or C++ | 2017 | Facebook, open source\n",
    "\n",
    "- See also [Comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n",
    "- Lots of new software.\n",
    "- Lots of Python.\n",
    "  \n",
    "Some nice things the software can do for you:\n",
    "\n",
    "- automatic differentiation\n",
    "- compile/optimize code, especially for GPU \n",
    "- numerically stable implementations\n",
    "- implementation of various regularizers (like dropout) and solvers (like Adam)\n",
    "- a common standard for a community of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs\n",
    "\n",
    "- A big part of the deep learning revolution\n",
    "- GPUs were originally designed for graphics --> _matrix multiplication_\n",
    "  - This is what we need for neural networks\n",
    "- Leader is NVIDIA, their GPU programming language is called CUDA.\n",
    "  - These days we can usually avoid writing in CUDA\n",
    "  - NVIDIA's share price 2 years ago: \\$35. Now: \\$220.\n",
    "  \n",
    "From Jeff Bezos' 2017 letter to shareholders:\n",
    "\n",
    "> Using our pre-packaged versions of popular deep learning frameworks running on P2 compute instances (optimized for this workload), customers are already developing powerful systems ranging everywhere from early disease detection to increasing crop yields. [...] Watch this space. Much more to come.\n",
    "\n",
    "-More recently: [P3 instances](https://aws.amazon.com/ec2/instance-types/p3/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud computing\n",
    "\n",
    "- CUDA-capable NVIDIA GPUs are [expensive](https://www.amazon.com/Nvidia-Tesla-GDDR5-Cores-Graphic/dp/B00Q7O7PQA). \n",
    "- Cloud computing platforms enable easy (and sometimes free) access. \n",
    "  - Big players are AWS EC2, Google Cloud, Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos\n",
    "\n",
    "The data is built in to Keras, so we just access it for convenience. If not present already it is automatically downloaded.\n",
    "\n",
    "Attribution: the code below is adapted from the [Keras MNIST example](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py), which is under the [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train_cat), (X_test, y_test_cat) = mnist.load_data()\n",
    "\n",
    "img_dim = (28,28) \n",
    "img_size = img_dim[0]*img_dim[1]\n",
    "num_classes = 10\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test  = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test  /= 255\n",
    "X_train_flat = X_train.reshape(60000, img_size)\n",
    "X_test_flat  = X_test.reshape(10000, img_size)\n",
    "X_train = X_train[...,None] # add 4th dimension, needed later for convnets\n",
    "X_test  = X_test[...,None]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train_cat, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test_cat, num_classes)\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgelbart/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9924\n",
      "Test accuracy: 0.9745\n",
      "25.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100,100), max_iter=10, \n",
    "                   batch_size=128)\n",
    "nn.fit(X_train_flat, y_train_cat)\n",
    "\n",
    "score = nn.score(X_train_flat, y_train_cat)\n",
    "print('Train accuracy:', score)\n",
    "\n",
    "score = nn.score(X_test_flat, y_test_cat)\n",
    "print('Test accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model definition **\n",
    "\n",
    "Here we need to specify the input and output size in advance (unlike sklearn) because the model is first _compiled_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_train_flat.shape[1],), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training and evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9924\n",
      "Test accuracy: 0.9764\n",
      "30.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "history = model.fit(X_train_flat, y_train,\n",
    "                    batch_size=128, \n",
    "                    epochs=10,\n",
    "                    verbose=0)\n",
    "\n",
    "score = model.evaluate(X_train_flat, y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional net\n",
    "\n",
    "Attribution: the code below is adapted from [Deep Learning with Python](https://machinelearningmastery.com/deep-learning-with-python2/) with permission from the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model definition **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (5, 5), input_shape=img_dim+(1,), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training and evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9781\n",
      "Test accuracy: 0.9774\n",
      "1min 4s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128, \n",
    "                    epochs=1,\n",
    "                    verbose=0)\n",
    "\n",
    "score = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Training in the cloud\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Go to https://colab.research.google.com/\n",
    "- Make an account\n",
    "- Upload notebook or create a new one\n",
    "- Runtime --> change runtime type\n",
    "- Select GPU\n",
    "\n",
    "Typical speedups of **10x**. This translates into more prototyping, more optimization, better models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
